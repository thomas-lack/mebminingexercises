
****** Clustering high-dimensional data ******

From Wikipedia, the free encyclopedia


Jump to: navigation, search

 This article needs attention from an expert on the subject. See the talk_page
 for details. WikiProject_Computer_science or the Computer_science_Portal may
 be able to help recruit an expert. (May 2009)
Clustering high-dimensional data is the cluster_analysis of data with anywhere
from a few dozen to many thousands of dimensions. Such high-dimensional data
spaces are often encountered in areas such as medicine, where DNA_microarray
technology can produce a large number of measurements at once, and the
clustering of text documents, where, if a word-frequency vector is used, the
number of dimensions equals the size of the dictionary.
***** Contents *****
    * 1_Problems
    * 2_Approaches
          o 2.1_Subspace_Clustering
          o 2.2_Projected_Clustering
          o 2.3_Hybrid_Approaches
          o 2.4_Correlation_Clustering
    * 3_References
***** [edit] Problems *****
According to Kriegel,_KrÃ¶ger_&amp;_Zimek_(2009), four problems need to be
overcome for clustering in high-dimensional data:
    * Multiple dimensions are hard to think in, impossible to visualize, and,
      due to the exponential growth of the number of possible values with each
      dimension, impossible to enumerate. This problem is known as the curse_of
      dimensionality.
    * The concept of distance becomes less precise as the number of dimensions
      grows, since the distance between any two points in a given dataset
      converges. The discrimination of the nearest and farthest point in
      particular becomes meaningless:
            [\lim_{d \to \infty} \frac{dist_\max - dist_\min}{dist_\min} \to 0]
    * A cluster is intended to group objects that are related, based on
      observations of their attribute's values. However, given a large number
      of attributes some of the attributes will usually not be meaningful for a
      given cluster. For example, in newborn_screening a cluster of samples
      might identify newborns that share similar blood values, which might lead
      to insights about the relevance of certain blood values for a disease.
      But for different diseases, different blood values might form a cluster,
      and other values might be uncorrelated. This is known as the local
      feature relevance problem: different clusters might be found in different
      subspaces, so a global filtering of attributes is not sufficient.
    * Given a large number of attributes, it is likely that some attributes are
      correlated. Hence, clusters might exist in arbitrarily oriented affine
      subspaces.
Recent research by Houle_et_al._(2010) indicates that the discrimination
problems only occur when there is a high number of irrelevant dimensions, and
that shared-nearest-neighbor approaches can improve results.
***** [edit] Approaches *****
Approaches towards clustering in axis-parallel or arbitrarily oriented affine
subspaces differ in how they interpret the overall goal, which is finding
clusters in data with high dimensionality. This distinction is proposed in
Kriegel,_KrÃ¶ger_&amp;_Zimek_(2009). An overall different approach is to find
clusters based on pattern in the data matrix, often referred to as
biclustering, which is a technique frequently utilized in bioinformatics.
**** [edit] Subspace Clustering ****
Example 2D space with subspace clusters
Subspace clustering is the task of detecting all clusters in all subspaces.
This means that a point might be a member of multiple clusters, each existing
in a different subspace. Subspaces can either be axis-parallel or affine. The
term is often used synonymous with general clustering in high-dimensional data.
The image on the right shows a mere two-dimensional space where a number of
clusters can be identified. In the one-dimensional subspaces, the clusters ca
(in subspace {x}) and cb, cc, cd (in subspace {y}) can be found. cc cannot be
considered a cluster in a two-dimensional (sub-)space, since it is too sparsely
distributed in the x axis. In two dimensions, the two clusters cab and cad can
be identified.
The problem of subspace clustering is given by the fact that there are 2d
different subspaces of a space with d dimensions. If the subspaces are not
axis-parallel, an infinite number of subspaces is possible. Hence, subspace
clustering algorithm utilize some kind of heuristic to remain computationally
feasible, at the risk of producing inferior results. For example, the downward-
closure property (cf. association_rules) can be used to build higher-
dimensional subspaces only by combining lower-dimensional ones, as any subspace
T containing a cluster, will result in a full space S also to contain that
cluster (i.e. S â T), an approach taken by most of the traditional algorithms
such as CLIQUE (Agrawal_et_al._2005) and SUBCLU (Kailing,_Kriegel_&amp;_KrÃ¶ger
2004).
**** [edit] Projected Clustering ****
Projected clustering seeks to assign each point to a unique cluster, but
clusters may exist in different subspaces. The general approach is to use a
special distance_function together with a regular clustering_algorithm.
For example, the PreDeCon algorithm checks which attributes seem to support a
clustering for each point, and adjusts the distance function such that
dimensions with low variance are amplified in the distance function (Bohm_et
al._2004). In the figure above, the cluster cc might be found using DBSCAN with
a distance function that places less emphasis on the x-axis and thus
exaggerates the low difference in the y-axis sufficiently enough to group the
points into a cluster.
PROCLUS uses a similar approach with a k-medoid clustering (Aggarwal_et_al.
1999). Initial medoids are guessed, and for each medoid the subspace spanned by
attributes with low variance is determined. Points are assigned to the medoid
closest, considering only the subspace of that medoid in determining the
distance. The algorithm then proceeds as the regular PAM algorithm.
If the distance function weights attributes differently, but never with 0 (and
hence never drops irrelevant attributes), the algorithm is called a "soft"-
projected clustering algorithm.
**** [edit] Hybrid Approaches ****
Not all algorithms try to either find a unique cluster assignment for each
point or all clusters in all subspaces; many settle for a result in between,
where a number of possibly overlapping, but not necessarily exhaustive set of
clusters are found. An example is FIRES, which is from its basic approach a
subspace clustering algorithm, but uses a heuristic too aggressive to credibly
produce all subspace clusters (Kriegel_et_al._2005).
**** [edit] Correlation Clustering ****
Another type of subspaces is considered in Correlation_clustering_(Data
Mining).
***** [edit] References *****
    * Aggarwal, Charu C.; Wolf, Joel L.; Yu, Philip S.; Procopiuc, Cecilia;
      Park, Jong Soo (1999), "Fast algorithms for projected clustering", ACM
      SIGMOD Record (New York, NY: ACM) 28 (2): 61â72, doi:10.1145/
      304181.304188 
    * Agrawal, Rakesh; Gehrke, Johannes; Gunopulos, Dimitrios; Raghavan,
      Prabhakar (2005), "Automatic Subspace Clustering of High Dimensional
      Data", Data Mining and Knowledge Discovery (Springer Netherlands) 11 (1):
      5â33, doi:10.1007/s10618-005-1396-1 
    * BÃ¶hm, Christian; Kailing, Karin; Kriegel, Hans-Peter; KrÃ¶ger, Peer
      (2004), "Density Connected Clustering with Local Subspace Preferences",
      Data Mining, IEEE International Conference on (Los Alamitos, CA, USA:
      IEEE Computer Society): 24â34, doi:10.1109/ICDM.2004.10087, ISBN 0-
      7695-2142-8 
    * Kailing, Karin; Kriegel, Hans-Peter; KrÃ¶ger, Peer (2004), "Density-
      Connected_Subspace_Clustering_for_High-Dimensional_Data", Proceedings of
      the Fourth SIAM International Conference on Data Mining (SIAM):
      246â257, http://www.dbs.informatik.uni-muenchen.de/Publikationen/
      Papers/sdm04-subclu.pdf, retrieved 2009-05-25 
    * Kriegel, Hans-Peter; KrÃ¶ger, Peer; Renz, Matthias; Wurst, Sebastian
      (2005), "A Generic Framework for Efficient Subspace Clustering of High-
      Dimensional Data", Proceedings of the Fifth IEEE International Conference
      on Data Mining (ICDM) (Washington, DC: IEEE Computer Society): 205â257,
      doi:10.1109/ICDM.2005.5, ISBN 0-7695-2278-5 
    * Kriegel, Hans-Peter; KrÃ¶ger, Peer; Zimek, Arthur (2009), "Clustering
      high-dimensional data: A survey on subspace clustering, pattern-based
      clustering, and correlation clustering", ACM Transactions on Knowledge
      Discovery from Data (New York, NY: ACM) 3 (1): 1â58, doi:10.1145/
      1497577.1497578 
    * Houle, Michael E.; Kriegel, Hans-Peter; KrÃ¶ger, Peer; Schubert, Erich;
      Zimek, Arthur (2010), "Can Shared-Neighbor Distances Defeat the Curse of
      Dimensionality?", Proceedings of the 21st International Conference on
      Scientific and Statistical Database Management (SSDBM) (Heidelberg,
      Germany: Springer) 

Retrieved from "http://en.wikipedia.org/wiki/Clustering_high-dimensional_data"

Categories: Data_analysis | Data_mining | Cluster_analysis
Hidden categories: Computer_science_articles_needing_expert_attention |
Articles_needing_expert_attention_from_May_2009 | All_articles_needing_expert
attention

